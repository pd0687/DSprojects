Objective: stock selection using machine learning classification algorithms, enhancing selection of each individual model via
voting classifier, bagging, and boosting, and then enhancing each model and ensemble by hyperparameter tuning at the model level

Approach:

-- each step below uses models' default hyperparameters, the observations are split 75/25 via train-test-split at each month
   from beginning (December 1994) to end (July 2015), and performance (i.e., accuracy) is logged by looping over each month

---- implement logsitic regression, k-nearest neighbors, and random forest classifiers using default hyperparameters and 75/25
     train-test-split
---- re-implement all three using default hyperparameters and 75/25 t-t-s, additionally performing ten-fold cross validation
     on the training set and gauging whether the validation set accuracy is in line with the test set accuracy
---- aggregate the results of all three models into a voting classifier using default hyperparameters and 75/25 t-t-s
---- implement a bagging classifier using the random forest as the base estimator and test each realization on out-of-bag
     observations
---- implement AdaBoost classifier using the random forest as the base estimator and test each realization on out-of-bag
     observations

-- hyperparameter-tune each of the logistic regression, k-nearest neighbors, and random forest classifiers by implementing
   GridSearchCV and performing ten-fold cross validation on the period from December 1994 to December 1996
   
-- over the period from January 1997 to July 2015, and again using 75/25 t-t-s, re-run as before and check if accuracy and/or
   variance have improved

Final results: pending



###################################################################################################################################
###################################################################################################################################
###################################################################################################################################
###################################################################################################################################



Chronology of submissions

7/9/2019

created capstone2 folder
submitted Capstone Project 2 -- Initial Project Ideas

###########################################################################################################

7/14/2019

submitted Capstone Project 2 -- Project Proposal
submitted data_wrangling notebook
submitted EDA notebook
submitted Capstone Project 2 -- Milestone Report 1

###########################################################################################################

7/20/2019

re-submitted EDA notebook

###########################################################################################################

7/21/2019

added information in the README regarding objective and approach of the project and the contents of each submission
submitted ML notebook
submitted CSV files that serve as dependencies for the data wrangling, EDA, and ML notebooks, respectively
submitted Capstone Project 2 -- Milestone Report 2



###################################################################################################################################
###################################################################################################################################
###################################################################################################################################
###################################################################################################################################



Contents of each submission:

Capstone Project 2 -- Initial Project Ideas Word document:

-- three project proposals:
---- stock selection using fundamental data (classification)
---- market timing using financial instruments' historical time series (PCA)
---- portfolio replication using a subset of instruments from the portfolio (clustering)

Capstone Project 2 -- Project Proposal Word document:

-- stock selection using fundamental data
-- implement supervised classification algorithms to distinguish future winners from future losers using accounting data and
   historical time series
-- implement ensemble techniques such as voting classifier, bagging, and boosting
-- hyperparameter-tune the underlying models in order to improve performance
-- performance is measured by accuracy (i.e., predict a winning stock and it does outperform, or predict a losing stock and it
   does underperform)
   
Capstone Project 2 -- Milestone Report 1 PDF:

-- status update
---- project objective
---- description of the dataset and how it was obtained, cleaned, and wrangled
---- initial findings from EDA

Capstone Project 2 -- Milestone Report 2 Word document:

-- status update
---- findings from the machine learning phase

data wrangling notebook:

-- Delete unnecessary columns after the CSV was imported as a dataframe
-- Check data types, and coerce the target to float
-- Verify that the dates are all end-of-month
-- Check to see how many nulls there are in each of the features and the target, check how many rows are free of nulls, and
   drop rows containing any nulls
-- Engineer new feature
-- Check if NaN in the new feature, and coerce NaN's to zero
-- Check if infinity entries in the new feature, and coerce to 9999
-- Rearrange/swap the last two columns so that the target is rightmost
-- Delete the feature that was used for engineering the new feature
-- Create function that groups by date and discretizes the (continuous) features into 5 quantiles
-- Create function that discretizes the target -- 1 if the next month's return lies above the median, 0 otherwise

EDA notebook:

-- Group the observations by date and compute the mean for all continuous features, then plot the grouped mean for each feature
-- Correlation matrix and heatmap for the continuous features for the ungrouped and the grouped (as above) dataframe
-- Simple linear regression where the continuous target is regressed on each of the continuous features
-- Histograms, skewness, and kurtosis for each of the continuous features and for the continuous target
-- Confidence intervals for each of the continuous features and for the continuous target
-- Subset the discrete target into "underperform" and "outperform," compute each observation's features' quantile average,
   then perform the difference of means test to see if outperforming stocks have features that belong to quantiles that are
   higher than those for underperforming stocks

ML notebook:

-- Loop through each of the dates, train and test, and report accuracy for each of the logistic regression, K-nearest neighbors,
   and random forest classifiers, with default hyperparameters
-- Plot each of the resulting accuracy curves over time
-- Sanity-check the test set accuracy by reiterating the first pass, but this time implementing ten-fold cross validation on
   the training set and comparing the accuracies between the validation and test sets
-- Implement ensemble models: voting classifier (using all three untuned models), bagging (using the untuned random forest
   classifier), and AdaBoost (using the untuned random forest classifier)
-- Tune the key hyperparameters for each of the three models by calling GridSearchCV with ten-fold CV over the first 25 months
   of data
-- Loop through the remaining N - 25 months (from January 1997 to July 2015) using the now-tuned logistic regression, KNN, and
   random forest classifiers, and report the accuracies
-- Over the same N - 25 months, re-run the voting classifier, bagging, and AdaBoost classifiers and report the accuracies
-- Compare and graph the accuracies between the tuned and untuned models
