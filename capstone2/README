Objective: stock selection using machine learning classification algorithms, enhancing selection of each individual model via
voting classifier, bagging, and boosting, and then enhancing each model and ensemble by hyperparameter tuning at the model level

Approach:

-- each step below uses models' default hyperparameters, the observations are split 75/25 via train-test-split at each month
   from beginning (December 1994) to end (July 2015), and performance (i.e., accuracy) is logged by looping over each month

---- implement logsitic regression, k-nearest neighbors, and random forest classifiers using default hyperparameters and 75/25
     train-test-split
---- re-implement all three using default hyperparameters and 75/25 t-t-s, additionally performing ten-fold cross validation
     on the training set and gauging whether the validation set accuracy is in line with the test set accuracy
---- aggregate the results of all three models into a voting classifier using default hyperparameters and 75/25 t-t-s
---- implement a bagging classifier using the random forest as the base estimator and test each realization on out-of-bag
     observations
---- implement AdaBoost classifier using the random forest as the base estimator and test each realization on out-of-bag
     observations

-- hyperparameter-tune each of the logistic regression, k-nearest neighbors, and random forest classifiers by implementing
   GridSearchCV and performing ten-fold cross validation on the period from December 1994 to December 1996
   
-- over the period from January 1997 to July 2015, and again using 75/25 t-t-s, re-run as before and check if accuracy and/or
   variance have improved

Final results: pending



###################################################################################################################################
###################################################################################################################################
###################################################################################################################################
###################################################################################################################################



Chronology of submissions

7/9/2019

created capstone2 folder
submitted Capstone Project 2 -- Initial Project Ideas

###########################################################################################################

7/14/2019

submitted Capstone Project 2 -- Project Proposal
submitted data_wrangling notebook
submitted EDA notebook
submitted Capstone Project 2 -- Milestone Report 1

###########################################################################################################

7/20/2019

re-submitted EDA notebook



###################################################################################################################################
###################################################################################################################################
###################################################################################################################################
###################################################################################################################################



Contents of each submission:

Capstone Project 2 -- Initial Project Ideas Word document:

-- three project proposals:
---- stock selection using fundamental data (classification)
---- market timing using financial instruments' historical time series (PCA)
---- portfolio replication using a subset of instruments from the portfolio (clustering)

Capstone Project 2 -- Project Proposal Word document:

-- stock selection using fundamental data
-- implement supervised classification algorithms to distinguish future winners from future losers using accounting data and
   historical time series
-- implement ensemble techniques such as voting classifier, bagging, and boosting
-- hyperparameter-tune the underlying models in order to improve performance
-- performance is measured by accuracy (i.e., predict a winning stock and it does outperform, or predict a losing stock and it
   does underperform)
   
Capstone Project 2 -- Milestone Report 1 PDF:

-- status update
---- project objective
---- description of the dataset and how it was obtained, cleaned, and wrangled
---- initial findings from EDA

data wrangling notebook:

-- Delete unnecessary columns after the CSV was imported as a dataframe
-- Check data types, and coerce the target to float
-- Verify that the dates are all end-of-month
-- Check to see how many nulls there are in each of the features and the target, check how many rows are free of nulls, and
   drop rows containing any nulls
-- Engineer new feature
-- Check if NaN in the new feature, and coerce NaN's to zero
-- Check if infinity entries in the new feature, and coerce to 9999
-- Rearrange/swap the last two columns so that the target is rightmost
-- Delete the feature that was used for engineering the new feature
-- Create function that groups by date and discretizes the (continuous) features into 5 quantiles
-- Create function that discretizes the target -- 1 if the next month's return lies above the median, 0 otherwise

EDA notebook:

-- Group the observations by date and compute the mean for all continuous features, then plot the grouped mean for each feature
-- Correlation matrix and heatmap for the continuous features for the ungrouped and the grouped (as above) dataframe
-- Simple linear regression where the continuous target is regressed on each of the continuous features
-- Histograms, skewness, and kurtosis for each of the continuous features and for the continuous target
-- Confidence intervals for each of the continuous features and for the continuous target
-- Subset the discrete target into "underperform" and "outperform," compute each observation's features' quantile average,
   then perform the difference of means test to see if outperforming stocks have features that belong to quantiles that are
   higher than those for underperforming stocks
